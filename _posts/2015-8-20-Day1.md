---
layout: post
comments: true
title: Day1 - Neural Networks and Backprob
excerpt: This excise will introduce the backpropagation algorithm which is the foundation for all modern Neural Networks
date: 2015-08-25T11:00:00.000Z
mathjax: true
published: true
---

##### Course Material 
* Exercise: [Github Repo](https://github.com/DTU-deeplearning/day1-NN) (open exercises_1.ipynb). Some of you have pointed out a bug in the sample solution (exercises_1_all_code.ipynb) stopping execution just before the dropoutlayer - it have been fixed now.
* Slides: Will be upladed at DTU CampusNet 
* Suggested Readings: Christopher M. Bishop “Pattern Recognition and Machine Learning”, chapter 5


##### Additional Material
* **[Feedforward NN in Lasagne tutorial](http://lasagne.readthedocs.org/en/latest/user/tutorial.html)** How to implement the a feedforward NN in Lasagne. 
* **[Backpropagation explabined by Michael Nielsen(Neural Networks and Deep Learning, chap 2)](http://neuralnetworksanddeeplearning.com/chap2.html)** Chaptor 2 gives a good and pretty elaborate explanation on how backpropagation works
* **[Neural networks class - Université de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)**: Neural Network Class by Hugo Larochelle.
* **[Deep learning at Oxford 2015](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)**: Neural Network Class by Nando de Freitas.
* **[Deep Learning](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)**: Recent Deep Learning review paper by Y. LeCun, Y. Bengio and G. Hinton.
* **[Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf)**: An influential paper on deeplearning by Hinton and Salakhutdinov that restarted the interest in neural networks. 
* **[Learning Internal Representations by Error Propagation](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)**: One of the original Backpropagation papers.
